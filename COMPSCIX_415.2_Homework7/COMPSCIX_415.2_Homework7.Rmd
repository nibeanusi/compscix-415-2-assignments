---
title: "COMPSCIX_415.2_Homework7"
author: "Nnenna Ibeanusi"
date: "7/29/2018"
output: html_document
---

```{r}
library("tidyverse")
```

#Exercise 1
```{r}
#Load the train.csv dataset into R. How many observations and columns are there?
train<- read_csv(file = 'train.csv')
```

```{r}
train
#1,460 observations, 81 variables
```

```{r}
dim(train)
```

```{r}
library("broom")
```


#Exercise 2
```{r}
#Visualize the distribution of SalePrice.
#Normally at this point you would spend a few days on EDA, but for this homework we will do some very basic EDA and get right to fitting some linear regression models.
train %>% ggplot(aes(x = SalePrice)) + geom_histogram() +
  theme_bw()
```


```{r}
#Visualize the covariation between SalePrice and Neighborhood.
train %>% ggplot(aes(x = Neighborhood, y = SalePrice)) +
  geom_boxplot() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90)) 
```


```{r}
#Visualize the covariation between SalePrice and OverallQual
train %>% ggplot(aes(x = OverallQual, y = SalePrice)) +
  geom_point() +
  theme_bw()
```

#Exercise 3
```{r}
#Our target is called SalePrice. First, we can fit a simple regression model consisting of only the intercept (the average of SalePrice). Fit the model and then use the broom package to
    # take a look at the coefficient,
    # compare the coefficient to the average value of SalePrice, and
    # take a look at the R-squared.
mod_0 <- lm(formula = SalePrice ~ 1, data = train)#Fit a model with an intercept onliy
```
```{r}
mod_0
```

```{r}
mean(train$SalePrice)#Double check the average SalesPrice is equal to our model's coefficient
```

```{r}
tidy(mod_0)#gives more statistics, uses tidy to clean it up 
```
```{r}
glance(mod_0)#Check the R-squared
```

#Exercise 4
```{r}
#Now fit a linear regression model using GrLivArea, OverallQual, and Neighborhood as the features. Don’t forget to look at data_description.txt to understand what these variables mean. Ask yourself these questions before fitting the model:

    # What kind of relationship will these features have with our target?
    # Can the relationship be estimated linearly?
    # Are these good features, given the problem we are trying to solve?
mod_1 <- lm(formula = SalePrice ~ GrLivArea + OverallQual + Neighborhood, data = train)
glance(mod_1)
```

```{r}
tidy(mod_1)
```

```{r}
# How would you interpret the coefficients on GrLivArea and OverallQual?
#For every extra square foot in the above ground living area, the sale price increases, on average, by $62, with all other features being held constant. 
#For every one unit increase in the overall quality of the house (as this value goes up, the quality is higher), the mean increase in the sale price is $21700.
```

```{r}
# How would you interpret the coefficient on NeighborhoodBrkSide?
#The mean difference in the sale price between Bloomington Heights and Brookside is -$14064, so Brookside is a less expensive area to live.
```
```{r}
# Are the features significant?
#Both GrLivArea and OverallQual are both statistically and practically significant. For neighborhood, many of the categories are significant (both statistically and practically), and some are not statistically significant.
```

```{r}
# Is the model a good fit?
# Based on the adjusted R-squared (.80), the model seems to be a good fit to the data.
```

```{r}
#One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below (use y as the target and x as the feature), and look at the resulting coefficients and R-squared. Rerun it about 5-6 times to generate different simulated datasets. What do you notice about the model’s coefficient on x and the R-squared values?
collect_results <- function(sim_data){
sim1a <- tibble(
  x = rep(1:10, each = 3),
  y = x * 1.5 + 6 + rt(length(x), df = 2)
  )
 sim_mod<- lm(formula = y ~ x, data = sim1a)
 rsq <- glance(sim_mod)%>% .[['adj.r.squared']]
 slopes <- tidy(sim_mod)%>% filter(term == 'x') %>% .[['estimate']]
  tibble(rsq, slopes)
}
# define iter to do 1000 simulations
iter <- 1:1000

# use the map_dfr function to map the function above to the iter
# vector. Basically, run the function 1000 times and collect all
# of the results into a data.frame
mod_results <- map_dfr(iter, collect_results)

# visualize the distributions of the adjusted R-squared and the slopes
mod_results %>% ggplot(aes(x = rsq)) +
  geom_histogram(color = 'white', fill = 'red') +
  labs(x = 'Adjusted R-squared', title = 'Distribution of model fit on simulated data') +
  theme_bw()
```

```{r}
mod_results %>% ggplot(aes(x = slopes)) +
  geom_histogram(color = 'white', fill = 'blue') +
  labs(x = 'Coefficient on x', title = 'Distribution of slopes for simulated data models') +
  theme_bw()
```

```{r}
mod_results
```

```{r}
#From the distributions of the adjusted R-squared and the slopes above, we can see that even when we simulate data from a known model, the slope and the model fit can vary widely simply because of slight changes in the data. This illustrates how sensitive regression can be.
```

